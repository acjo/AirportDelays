{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Names: Bryce Hepner, Caelan Osman, Carter Landon\n",
    "Project: We are trying to predict when flights will be late based on a dataset found from Kaggle.\n",
    "The dataset includes flight data from 2016 and 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'recall_score' from 'sklearn.model_selection' (C:\\Users\\hepne\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-24178ed8a3a6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtree\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mexport_graphviz\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGridSearchCV\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrecall_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfilterwarnings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ignore'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'recall_score' from 'sklearn.model_selection' (C:\\Users\\hepne\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def data_cleaning():\n",
    "    '''This function cleans the data we will be using\n",
    "    :return:\n",
    "    flight_2016: pandas dataframe with the cleaned flight data from 2016\n",
    "    flight_2017: pandas dataframe with the cleaned fligth data from 2017\n",
    "    '''\n",
    "    flight_2016 = pd.read_csv('flight.csv', delimiter=',')\n",
    "    #drop useless flight data\n",
    "    flight_2016.drop(['Month', 'Year', 'Day', 'Flight_Date', 'FlightNum',\n",
    "                      'Departure_Time', 'DepDel15', 'Dep_Delay_Groups',\n",
    "                      'Arrival_Time', 'Arrival_Delay', 'Arr_Delay_Minutes',\n",
    "                      'Arr_Del_morethan15', 'Cancelled', 'Diverted',\n",
    "                      'DistanceGroup', 'Carrier_Delay', 'WeatherDelay', 'NAS_Delay',\n",
    "                      'Security_Delay', 'Late_Aircraft_Delay', 'Top_Carriers', 'Top_Origin',\n",
    "                      'DEPTIME_GROUP1', 'DEPTIME_GROUP2', 'DEPTIME_GROUP3' ], axis=1, inplace=True)\n",
    "\n",
    "    flight_2017 = pd.read_csv('fl_samp.csv', delimiter=',')\n",
    "    #drop useless flight data\n",
    "    flight_2017.drop(['Year', 'Month', 'Day', 'Flight_Date', 'UniqueCarrier', 'Departure_Time',\n",
    "                      'Scheduled_Arrival', 'Arrival_Delay', 'Arr_Del_morethan15', 'DistanceGroup',\n",
    "                      'Carrier_Delay', 'WeatherDelay', 'NAS_Delay', 'Late_Aircraft_Delay',\n",
    "                      'DEPTIME_GROUP1', 'DEPTIME_GROUP2', 'DEPTIME_GROUP3' ], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return flight_2016, flight_2017\n",
    "\n",
    "def plot_data():\n",
    "    '''Creates some plots of the data from 2016 and 2017\n",
    "    '''\n",
    "    flight_2016, flight_2017 = data_cleaning()\n",
    "    #plot 2016 histogram\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2016['Dep_Delay'], color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2016')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2016 histogram with log scale\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2016['Dep_Delay'], log=True, bins=10, color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2016 log scale')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2017 histogram\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2017['Dep_Delay'], color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2017')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2017 histogram with log scale\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2017['Dep_Delay'], log=True, bins=10, color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2017 log scale')\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def smote(X,N,k):\n",
    "    \"\"\" Generate synthetic points using the SMOTE method.\n",
    "    Parameters:\n",
    "        X (n,m): minority class samples\n",
    "        N (int): number of samples to generate from each point\n",
    "        k (int): number of nearest neighbors\n",
    "    Returns:\n",
    "        synthetic ndarray(N*n,m): synthetic minority class samples\n",
    "    \"\"\"\n",
    "    # the number of columns in the number features and\n",
    "    # the number of rows is the number of observations (points)\n",
    "    n, m = X.shape\n",
    "    synthetic_samples = np.zeros((N*n, m))\n",
    "    #create tree\n",
    "    tree = KDTree(X)\n",
    "    for i in range(m):\n",
    "        #get k nearest neighbors for the current row\n",
    "        dist, indices = tree.query(X[i:i+1], k=k)\n",
    "        #now we have to create our new samples\n",
    "        for j in range(N):\n",
    "            #random choice of the nearest neighbors\n",
    "            neighbor = X[indices[0][np.random.randint(len(indices[0]))]]\n",
    "            #now generate a random point that lies between the two original values\n",
    "            random_point = np.random.uniform(0, 1, m)\n",
    "            #set the row of the new array\n",
    "            synthetic_samples[i*N+j] = X[i] + (neighbor - X[i])*random_point\n",
    "\n",
    "    return synthetic_samples\n",
    "def train_test_data(train_size=0.7, binary=True):\n",
    "    ''' This function takes in the flight data from 2016 and returns a train_test_split of the data\n",
    "    :param flight_2016: pandas dataframe containing data\n",
    "    :param train_size: the amount of data to test on defualts to a 70-30 train test split\n",
    "    :param smote: parameter to include smote data (to augment points with large delay times that\n",
    "                  may be infrequent).\n",
    "    :return X_train, X_test, y_train, y_test:\n",
    "    '''\n",
    "    flight_2016, _  = data_cleaning()\n",
    "    if binary:\n",
    "        #create the binary labels for if you were late or not\n",
    "\n",
    "        mask_on_time = flight_2016['Dep_Delay'] <= 0\n",
    "        flight_2016 = flight_2016.assign(Delay=lambda x: flight_2016.Dep_Delay *0)\n",
    "        flight_2016['Delay'][mask_on_time] = 0\n",
    "        flight_2016['Delay'][~mask_on_time] = 1\n",
    "        y = flight_2016['Delay']\n",
    "        X = flight_2016.drop(['Dep_Delay', 'Delay'], axis=1)\n",
    "        #traint test split on the binary data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            train_size=train_size,\n",
    "                                                            random_state=42)\n",
    "    else:\n",
    "        #create appropriate masks\n",
    "        mask_on_time = flight_2016['Dep_Delay'] <=0\n",
    "        mask_15_late = (flight_2016['Dep_Delay'] > 0) & (flight_2016['Dep_Delay'] <=15)\n",
    "        mask_30_late = (flight_2016['Dep_Delay'] > 15) & (flight_2016['Dep_Delay'] <=30)\n",
    "        mask_45_late = (flight_2016['Dep_Delay'] > 30) & (flight_2016['Dep_Delay'] <=45)\n",
    "        mask_60_late = (flight_2016['Dep_Delay'] > 45) & (flight_2016['Dep_Delay'] <=60)\n",
    "        mask_120_late = (flight_2016['Dep_Delay'] > 60) & (flight_2016['Dep_Delay'] <=120)\n",
    "        mask_180_late = (flight_2016['Dep_Delay'] > 120) & (flight_2016['Dep_Delay'] <=180)\n",
    "        mask_240_late = (flight_2016['Dep_Delay'] > 180) & (flight_2016['Dep_Delay'] <=240)\n",
    "        mask_300_late = (flight_2016['Dep_Delay'] > 240) & (flight_2016['Dep_Delay'] <=300)\n",
    "        mask_400_late = (flight_2016['Dep_Delay'] > 300) & (flight_2016['Dep_Delay'] <=400)\n",
    "        mask_500_late = (flight_2016['Dep_Delay'] > 400) & (flight_2016['Dep_Delay'] <=500)\n",
    "        mask_600_late = (flight_2016['Dep_Delay'] > 500) & (flight_2016['Dep_Delay'] <=600)\n",
    "        mask_700_late = (flight_2016['Dep_Delay'] > 600) & (flight_2016['Dep_Delay'] <=700)\n",
    "        mask_800_late = (flight_2016['Dep_Delay'] > 700) & (flight_2016['Dep_Delay'] <=800)\n",
    "        mask_900_late = (flight_2016['Dep_Delay'] > 800) & (flight_2016['Dep_Delay'] <=900)\n",
    "        mask_1000_late = (flight_2016['Dep_Delay'] > 900) & (flight_2016['Dep_Delay'] <=1000)\n",
    "        mask_1000_or_more_late = flight_2016['Dep_Delay'] > 1000\n",
    "        flight_2016 = flight_2016.assign(Delay=lambda x: flight_2016.Dep_Delay *0)\n",
    "\n",
    "        flight_2016['Delay'][mask_on_time.values] = 'on time'\n",
    "        flight_2016['Delay'][mask_15_late] = '15 minutes'\n",
    "        flight_2016['Delay'][mask_30_late] = '30 minutes'\n",
    "        flight_2016['Delay'][mask_45_late] = '45 minutes'\n",
    "        flight_2016['Delay'][mask_60_late] = '60 minutes'\n",
    "        flight_2016['Delay'][mask_120_late] = '120 minutes'\n",
    "        flight_2016['Delay'][mask_180_late] = '180 minutes'\n",
    "        flight_2016['Delay'][mask_240_late] = '240 minutes'\n",
    "        flight_2016['Delay'][mask_300_late] = '300 minutes'\n",
    "        flight_2016['Delay'][mask_400_late] = '400 minutes'\n",
    "        flight_2016['Delay'][mask_500_late] = '500 minutes'\n",
    "        flight_2016['Delay'][mask_600_late] = '600 minutes'\n",
    "        flight_2016['Delay'][mask_700_late] = '700 minutes'\n",
    "        flight_2016['Delay'][mask_800_late] = '800 minutes'\n",
    "        flight_2016['Delay'][mask_900_late] = '900 minutes'\n",
    "        flight_2016['Delay'][mask_1000_late] = '1000 minutes'\n",
    "        flight_2016['Delay'][mask_1000_or_more_late] = 'More than 1000 minutes'\n",
    "        y = flight_2016['Delay']\n",
    "        X = flight_2016.drop(['Dep_Delay', 'Delay'], axis=1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)\n",
    "\n",
    "        pass\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def best_kNN(binary):\n",
    "    '''Calculates the best hyperparameters for the KNeightborsClassifier, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    neighborclassifier = KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors':[2,10], 'weights': ('uniform','distance'), \\\n",
    "        'leaf_size':[20,50], \"p\":(1,2), \"n_jobs\":(-1)}\n",
    "    gridsearch = GridSearchCV(neighborclassifier, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "\n",
    "def best_logistic(binary):\n",
    "    '''Calculates the best hyperparameters for the LogisticRegression, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    logisticregression = LogisticRegression()\n",
    "    parameters = {'penalty':('l1', 'l2', 'elasticnet', 'none'), 'tol':(1e-6,1e-5,1e-4,1e-3,1e-2),\\\n",
    "        'C':(.1,.3,.5,.8,1,1.2,1.5,1.8), \"fit_intercept\":(False,True), \"n_jobs\":(-1)}\n",
    "    gridsearch = GridSearchCV(logisticregression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "\n",
    "def best_elastic(binary):\n",
    "    '''Calculates the best hyperparameters for the ElasticRegression, then uses those to\n",
    "    predict the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    elastic_regression = ElasticNet()\n",
    "    parameters = {'alpha':(.5,.8,1,1.2,1.5), 'l1_ratio':(.2,.3,.4,.5,.6,.7,.8),\\\n",
    "        'fit_intercept':(True,False), \"normalize\":(False,True), \"n_jobs\":(-1)}\n",
    "    gridsearch = GridSearchCV(elastic_regression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "\n",
    "def best_random_forest_reg(binary):\n",
    "    '''Calculates the best hyperparameters for the RandomForestRegression, then uses those to\n",
    "    predict the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    random_forest_regression = RandomForestRegressor()\n",
    "    parameters = {'n_estimators':(10,50,100,500,1000), 'criterion':(\"squared_error\",\"absolute_error\",\"poisson\"),\\\n",
    "        'max_depth':[5,20], 'bootstrap':(True,False), \"n_jobs\":(-1)}\n",
    "    gridsearch = GridSearchCV(random_forest_regression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "\n",
    "def best_random_forest_class(binary):\n",
    "    '''Calculates the best hyperparameters for the RandomForestRegression, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    random_forest_class = RandomForestClassifier()\n",
    "    parameters = {'n_estimators':(10,50,100,500,1000), 'criterion':(\"gini\", \"entropy\"),\\\n",
    "        'max_depth':[5,20], 'bootstrap':(True,False), \"n_jobs\":(-1)}\n",
    "    gridsearch = GridSearchCV(random_forest_class, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "\n",
    "def best_Gaussian(binary):\n",
    "    '''Calculates the best hyperparameters for the RandomForestRegression, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            flight_2016 (Pandas Dataframe): Any data really, but in this case the flight data\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    X_train,X_test,y_train,y_test = train_test_data(train_size=0.7, binary=binary)\n",
    "    random_forest_class = GaussianNB()\n",
    "    parameters = {'var_smoothing': (1e-10,1e-9,1e-8)}\n",
    "    gridsearch = GridSearchCV(random_forest_class, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    recall = recall_score(y_test,prediction)\n",
    "    return best_score, recall, best_params\n",
    "#kNN\n",
    "#NaiveBayes\n",
    "#RandomForrest\n",
    "#LogisticRegression\n",
    "#OLS\n",
    "#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    #flight_2016, flight_2017 = data_cleaning()\n",
    "    #print(pd.unique(flight_2016['Dep_Delay']))\n",
    "    #print(flight_2016['Dep_Delay'].value_counts())\n",
    "    X_train, X_test, y_train, y_test = train_test_data(train_size=0.7, binary=False)\n",
    "    print(best_kNN(True))\n",
    "    print(best_kNN(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_kNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-89ca384fe1bf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbest_kNN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbest_kNN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_kNN' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_kNN(True))\n",
    "print(best_kNN(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}