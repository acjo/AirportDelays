{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Names: Bryce Hepner, Caelan Osman, Carter Landon\n",
    "Project: We are trying to predict when flights will be late based on a dataset found from Kaggle.\n",
    "The dataset includes flight data from 2016 and 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from sklearn.tree import export_graphviz\n",
    "\n",
    "def data_cleaning():\n",
    "    '''This function cleans the data we will be using\n",
    "    :return:\n",
    "    flight_2016: pandas dataframe with the cleaned flight data from 2016\n",
    "    flight_2017: pandas dataframe with the cleaned fligth data from 2017\n",
    "    '''\n",
    "    flight_2016 = pd.read_csv('flight.csv', delimiter=',')\n",
    "    #drop useless flight data\n",
    "    flight_2016.drop(['Month', 'Year', 'Day', 'Flight_Date', 'FlightNum',\n",
    "                      'Departure_Time', 'DepDel15', 'Dep_Delay_Groups',\n",
    "                      'Arrival_Time', 'Arrival_Delay', 'Arr_Delay_Minutes',\n",
    "                      'Arr_Del_morethan15', 'Cancelled', 'Diverted',\n",
    "                      'DistanceGroup', 'UniqueCarrier', 'Carrier_Delay', 'WeatherDelay', 'NAS_Delay',\n",
    "                      'Security_Delay', 'Late_Aircraft_Delay', 'Top_Carriers', 'Top_Origin',\n",
    "                      'DEPTIME_GROUP1', 'DEPTIME_GROUP2', 'DEPTIME_GROUP3' , 'Tai_lNum', 'Origin_City_Name', 'Origin_State'], axis=1, inplace=True)\n",
    "\n",
    "    flight_2017 = pd.read_csv('fl_samp.csv', delimiter=',')\n",
    "    #drop useless flight data\n",
    "    flight_2017.drop(['Year', 'Month', 'Day', 'Flight_Date', 'UniqueCarrier', 'Departure_Time',\n",
    "                      'Scheduled_Arrival', 'Arrival_Delay', 'Arr_Del_morethan15', 'DistanceGroup',\n",
    "                      'Carrier_Delay', 'WeatherDelay', 'NAS_Delay', 'Late_Aircraft_Delay',\n",
    "                      'DEPTIME_GROUP1', 'DEPTIME_GROUP2', 'DEPTIME_GROUP3' ], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return flight_2016, flight_2017\n",
    "\n",
    "def plot_data():\n",
    "    '''Creates some plots of the data from 2016 and 2017\n",
    "    '''\n",
    "    flight_2016, flight_2017 = data_cleaning()\n",
    "    #plot 2016 histogram\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2016['Dep_Delay'], color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2016')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2016 histogram with log scale\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2016['Dep_Delay'], log=True, bins=10, color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2016 log scale')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2017 histogram\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2017['Dep_Delay'], color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2017')\n",
    "    plt.show()\n",
    "\n",
    "    #plot 2017 histogram with log scale\n",
    "    fig = plt.figure()\n",
    "    fig.set_dpi(150)\n",
    "    plt.hist(flight_2017['Dep_Delay'], log=True, bins=10, color='skyblue', ec='black' )\n",
    "    plt.title('Departure delay times from 2017 log scale')\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def smote(X,N,k):\n",
    "    \"\"\" Generate synthetic points using the SMOTE method.\n",
    "    Parameters:\n",
    "        X (n,m): minority class samples\n",
    "        N (int): number of samples to generate from each point\n",
    "        k (int): number of nearest neighbors\n",
    "    Returns:\n",
    "        synthetic ndarray(N*n,m): synthetic minority class samples\n",
    "    \"\"\"\n",
    "    # the number of columns in the number features and\n",
    "    # the number of rows is the number of observations (points)\n",
    "    #n, m = X.shape\n",
    "    #synthetic_samples = np.zeros((N*n, m))\n",
    "    #create tree\n",
    "    #tree = KDTree(X)\n",
    "    # Sorry Caelan I'm sure your code worked great for you but I'm going to \n",
    "    # replace it with my own\n",
    "    # Create the KD Tree for use\n",
    "    tree = KDTree(X)\n",
    "    \n",
    "    # Add the synthetic minorities\n",
    "    synth = []\n",
    "    n = X.shape[1]\n",
    "    for sample in X:\n",
    "        # Get the k nearest neighbors\n",
    "        dist, nearest = tree.query(sample.reshape(1,-1), k=k)\n",
    "        for _ in range(N):\n",
    "            # Select a neighbor among the nearest\n",
    "            selected = X[np.random.choice(nearest[0])]\n",
    "            \n",
    "            # Add a point between the two\n",
    "            lower = [min(selected[i],sample[i]) for i in range(4)]\n",
    "            upper = [max(selected[i],sample[i]) for i in range(4)]\n",
    "            \n",
    "            # Deal with the one-hot-encoded columns correctly\n",
    "            to_append = np.random.uniform(lower,upper)\n",
    "            if np.random.choice([0,1]) == 1:\n",
    "                to_append = np.concatenate((to_append,sample[4:]))\n",
    "            else:\n",
    "                to_append = np.concatenate((to_append,selected[4:]))\n",
    "            #to_append[-1] = sample[-1]\n",
    "            synth.append(to_append)\n",
    "    return np.array(synth)\n",
    "\n",
    "def train_test_data(train_size=0.7, binary=False, smote_data=True):\n",
    "    ''' This function takes in the flight data from 2016 and returns a train_test_split of the data\n",
    "    :param flight_2016: pandas dataframe containing data\n",
    "    :param train_size: the amount of data to test on defualts to a 70-30 train test split\n",
    "    :param smote: parameter to include smote data (to augment points with large delay times that\n",
    "                  may be infrequent).\n",
    "    :return X_train, X_test, y_train, y_test:\n",
    "    '''\n",
    "    flight_2016, _  = data_cleaning()\n",
    "    #one hot encode\n",
    "    flight_2016 = pd.get_dummies(flight_2016, columns=['Origin_Airport'], drop_first=True)\n",
    "    if binary:\n",
    "        #create the binary labels for if you were late or not\n",
    "\n",
    "        mask_on_time = flight_2016['Dep_Delay'] <= 0\n",
    "        flight_2016 = flight_2016.assign(Delay=lambda x: flight_2016.Dep_Delay *0)\n",
    "        flight_2016['Delay'][mask_on_time] = 0\n",
    "        flight_2016['Delay'][~mask_on_time] = 1\n",
    "        y = flight_2016['Delay']\n",
    "        X = flight_2016.drop(['Dep_Delay', 'Delay'], axis=1)\n",
    "        #traint test split on the binary data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            train_size=train_size,\n",
    "                                                            random_state=0)\n",
    "    else:\n",
    "        #create appropriate masks\n",
    "        mask_on_time = flight_2016['Dep_Delay'] <=0\n",
    "        mask_15_late = (flight_2016['Dep_Delay'] > 0) & (flight_2016['Dep_Delay'] <=15)\n",
    "        mask_30_late = (flight_2016['Dep_Delay'] > 15) & (flight_2016['Dep_Delay'] <=30)\n",
    "        mask_45_late = (flight_2016['Dep_Delay'] > 30) & (flight_2016['Dep_Delay'] <=45)\n",
    "        mask_60_late = (flight_2016['Dep_Delay'] > 45) & (flight_2016['Dep_Delay'] <=60)\n",
    "        mask_120_late = (flight_2016['Dep_Delay'] > 60) & (flight_2016['Dep_Delay'] <=120)\n",
    "        mask_180_late = (flight_2016['Dep_Delay'] > 120) & (flight_2016['Dep_Delay'] <=180)\n",
    "        mask_240_late = (flight_2016['Dep_Delay'] > 180) & (flight_2016['Dep_Delay'] <=240)\n",
    "        mask_300_late = (flight_2016['Dep_Delay'] > 240) & (flight_2016['Dep_Delay'] <=300)\n",
    "        mask_400_late = (flight_2016['Dep_Delay'] > 300) & (flight_2016['Dep_Delay'] <=400)\n",
    "        mask_500_late = (flight_2016['Dep_Delay'] > 400) & (flight_2016['Dep_Delay'] <=500)\n",
    "        mask_600_late = (flight_2016['Dep_Delay'] > 500) & (flight_2016['Dep_Delay'] <=600)\n",
    "        mask_700_late = (flight_2016['Dep_Delay'] > 600) & (flight_2016['Dep_Delay'] <=700)\n",
    "        mask_800_late = (flight_2016['Dep_Delay'] > 700) & (flight_2016['Dep_Delay'] <=800)\n",
    "        mask_900_late = (flight_2016['Dep_Delay'] > 800) & (flight_2016['Dep_Delay'] <=900)\n",
    "        mask_1000_late = (flight_2016['Dep_Delay'] > 900) & (flight_2016['Dep_Delay'] <=1000)\n",
    "        mask_1000_or_more_late = flight_2016['Dep_Delay'] > 1000\n",
    "        flight_2016 = flight_2016.assign(Delay=lambda x: flight_2016.Dep_Delay *0)\n",
    "\n",
    "        flight_2016['Delay'][mask_on_time] = 0\n",
    "        flight_2016['Delay'][mask_15_late] = 15\n",
    "        flight_2016['Delay'][mask_30_late] = 30\n",
    "        flight_2016['Delay'][mask_45_late] = 40\n",
    "        flight_2016['Delay'][mask_60_late] = 60\n",
    "        flight_2016['Delay'][mask_120_late] = 120\n",
    "        flight_2016['Delay'][mask_180_late] = 180\n",
    "        flight_2016['Delay'][mask_240_late] = 240\n",
    "        flight_2016['Delay'][mask_300_late] = 300\n",
    "        flight_2016['Delay'][mask_400_late] = 400\n",
    "        flight_2016['Delay'][mask_500_late] = 500\n",
    "        flight_2016['Delay'][mask_600_late] = 600\n",
    "        flight_2016['Delay'][mask_700_late] = 700\n",
    "        flight_2016['Delay'][mask_800_late] = 800\n",
    "        flight_2016['Delay'][mask_900_late] = 900\n",
    "        flight_2016['Delay'][mask_1000_late] = 1000\n",
    "        flight_2016['Delay'][mask_1000_or_more_late] = 10000\n",
    "        y = flight_2016['Delay']\n",
    "        X = flight_2016.drop(['Dep_Delay', 'Delay'], axis=1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            train_size=train_size, \n",
    "                                                            random_state=0)\n",
    "        if smote_data:\n",
    "\n",
    "            #smote data for 240 minutes\n",
    "            mask = y_train == 240\n",
    "            train_240 = X_train[mask]\n",
    "            smote_240 = smote(train_240.to_numpy(), 10, 2)\n",
    "            smote_240_df = pd.DataFrame(smote_240, columns=X_train.columns)\n",
    "            smote_240_labels = 240*np.ones(smote_240_df.shape[0]).astype(int)\n",
    "            smote_240_labels_df = pd.Series(smote_240_labels)\n",
    "            X_train = pd.concat([X_train, smote_240_df])\n",
    "            y_train = pd.concat([y_train, smote_240_labels_df])\n",
    "\n",
    "            #smote data for 300 minutes\n",
    "            mask = y_train == 300\n",
    "            train_300 = X_train[mask]\n",
    "            smote_300 = smote(train_300.to_numpy(), 10, 2)\n",
    "            smote_300_df = pd.DataFrame(smote_300, columns=X_train.columns)\n",
    "            smote_300_labels = 300*np.ones(smote_300_df.shape[0]).astype(int)\n",
    "            smote_300_labels_df = pd.Series(smote_300_labels)\n",
    "            X_train = pd.concat([X_train, smote_300_df])\n",
    "            y_train = pd.concat([y_train, smote_300_labels_df])\n",
    "\n",
    "            #smote data for 400 minutes\n",
    "            mask = y_train == 400\n",
    "            train_400 = X_train[mask]\n",
    "            smote_400 = smote(train_400.to_numpy(), 10, 2)\n",
    "            smote_400_df = pd.DataFrame(smote_400, columns=X_train.columns)\n",
    "            smote_400_labels = 400*np.ones(smote_400_df.shape[0]).astype(int)\n",
    "            smote_400_labels_df = pd.Series(smote_400_labels)\n",
    "            X_train = pd.concat([X_train, smote_400_df])\n",
    "            y_train = pd.concat([y_train, smote_400_labels_df])\n",
    "\n",
    "            #smote data for 500 minutes (zero planes are late by 500 minutes so ignore)\n",
    "            #smote data for 600 minutes\n",
    "            mask = y_train == 600\n",
    "            train_600 = X_train[mask]\n",
    "            smote_600 = smote(train_600.to_numpy(), 99, 1)\n",
    "            smote_600_df = pd.DataFrame(smote_600, columns=X_train.columns)\n",
    "            smote_600_labels = 600*np.ones(smote_600_df.shape[0]).astype(int)\n",
    "            smote_600_labels_df = pd.Series(smote_600_labels)\n",
    "            X_train = pd.concat([X_train, smote_600_df])\n",
    "            y_train = pd.concat([y_train, smote_600_labels_df])\n",
    "\n",
    "\n",
    "            '''\n",
    "            #smote data for 700 minutes (zero plantes are late by 700 minutes so ignore)\n",
    "            #smote data for 800 minutes, only 1 datapoint\n",
    "            mask = y_train == 800\n",
    "            print(sum(mask))\n",
    "            train_800 = X_train[mask]\n",
    "            smote_800 = smote(train_800.to_numpy(), 200, 1)\n",
    "            smote_800_df = pd.DataFrame(smote_800, columns=X_train.columns)\n",
    "            smote_800_labels = 800*np.ones(smote_800_df.shape[0]).astype(int)\n",
    "            smote_800_labels_df = pd.Series(smote_800_labels)\n",
    "            X_train = pd.concat([X_train, smote_800_df])\n",
    "            y_train = pd.concat([y_train, smote_800_labels_df])\n",
    "\n",
    "            #smote data for 900 minutes, only 1 datapoint\n",
    "            mask = y_train == 900\n",
    "            print(sum(mask))\n",
    "            train_900 = X_train[mask]\n",
    "            smote_900 = smote(train_900.to_numpy(), 120, 1)\n",
    "            smote_900_df = pd.DataFrame(smote_900, columns=X_train.columns)\n",
    "            smote_900_labels = 900*np.ones(smote_900_df.shape[0]).astype(int)\n",
    "            smote_900_labels_df = pd.Series(smote_900_labels)\n",
    "            X_train = pd.concat([X_train, smote_900_df])\n",
    "            y_train = pd.concat([y_train, smote_900_labels_df])\n",
    "\n",
    "            #smote data for 1000 minutes, only 1 datapoint\n",
    "            mask = y_train == 1000\n",
    "            train_1000 = X_train[mask]\n",
    "            smote_1000 = smote(train_1000.to_numpy(), 120, 1)\n",
    "            smote_1000_df = pd.DataFrame(smote_1000, columns=X_train.columns)\n",
    "            smote_1000_labels = 1000*np.ones(smote_1000_df.shape[0]).astype(int)\n",
    "            smote_1000_labels_df = pd.Series(smote_1000_labels)\n",
    "            X_train = pd.concat([X_train, smote_1000_df])\n",
    "            y_train = pd.concat([y_train, smote_1000_labels_df])\n",
    "            '''\n",
    "\n",
    "\n",
    "            #smote data for more than 1000 minutes\n",
    "            mask = y_train == 10000\n",
    "            train_10000 = X_train[mask]\n",
    "            smote_10000 = smote(train_10000.to_numpy(), 120, 1)\n",
    "            smote_10000_df = pd.DataFrame(smote_10000, columns=X_train.columns)\n",
    "            smote_10000_labels = 1000*np.ones(smote_10000_df.shape[0]).astype(int)\n",
    "            smote_10000_labels_df = pd.Series(smote_10000_labels)\n",
    "            X_train = pd.concat([X_train, smote_10000_df])\n",
    "            y_train = pd.concat([y_train, smote_10000_labels_df])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def best_kNN(X_train, X_test, y_train, y_test,binary):\n",
    "    '''Calculates the best hyperparameters for the KNeightborsClassifier, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "\n",
    "    neighborclassifier = KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors':[2,4], 'weights': ('uniform','distance'), \\\n",
    "        'leaf_size':(20,30,40,50), \"p\":(1,2), \"n_jobs\":[-1]}\n",
    "    gridsearch = GridSearchCV(neighborclassifier, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "\n",
    "def best_logistic(X_train, X_test, y_train, y_test,binary):\n",
    "    '''Calculates the best hyperparameters for the LogisticRegression, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "\n",
    "    logisticregression = LogisticRegression()\n",
    "    parameters = {'penalty':('l1', 'l2', 'elasticnet', 'none'), 'tol':[1e-3],\\\n",
    "        'C':(.1,.3,.5,.8,1,1.2,1.5,1.8), \"fit_intercept\":(False,True), \"n_jobs\":[-1], 'max_iter':[400]}\n",
    "    gridsearch = GridSearchCV(logisticregression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "\n",
    "def best_elastic(X_train, X_test, y_train, y_test,binary):\n",
    "    '''Calculates the best hyperparameters for ElasticRegression, then uses those to\n",
    "    predict the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "\n",
    "    elastic_regression = ElasticNet()\n",
    "    parameters = {'alpha':(.5,.8,1,1.2,1.5), 'l1_ratio':(.2,.3,.4,.5,.6,.7,.8),\\\n",
    "        'fit_intercept':(True,False), \"normalize\":(False,True)}\n",
    "    gridsearch = GridSearchCV(elastic_regression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "\n",
    "def best_random_forest_reg(X_train, X_test, y_train, y_test,binary):\n",
    "    '''Calculates the best hyperparameters for RandomForestRegression, then uses those to\n",
    "    predict the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    random_forest_regression = RandomForestClassifier()\n",
    "    parameters = {'n_estimators':[100], 'criterion':(\"squared_error\",\"absolute_error\",\"poisson\"),\\\n",
    "        'max_depth':(5,10,15,20), 'bootstrap':(True,False), \"n_jobs\":[-1]}\n",
    "    gridsearch = GridSearchCV(random_forest_regression, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "\n",
    "def best_random_forest_class(X_train, X_test, y_train, y_test,binary):\n",
    "    '''Calculates the best hyperparameters for the RandomForestClassifier, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "\n",
    "    random_forest_class = RandomForestClassifier()\n",
    "    parameters = {'n_estimators':[500], 'criterion':(\"gini\", \"entropy\"),\\\n",
    "        'max_depth':(3,4,5,6,7), 'bootstrap':(True,False), \"n_jobs\":[-1]}\n",
    "    gridsearch = GridSearchCV(random_forest_class, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "\n",
    "def best_Gaussian(X_train, X_test, y_train, y_test, binary):\n",
    "    '''Calculates the best hyperparameters for the GaussianNB, then uses those to\n",
    "    classify the data\n",
    "        Parameters:\n",
    "            binary (binary): Use the late binary or not\n",
    "        Returns:\n",
    "            best_score (float) best accuracy from the data\n",
    "            recall (recall) best recall score from the data \n",
    "            hyperparameters (dictionary) best hyperparameters from the data'''\n",
    "    random_forest_class = GaussianNB()\n",
    "    parameters = {'var_smoothing': (1e-10,1e-9,1e-8)}\n",
    "    gridsearch = GridSearchCV(random_forest_class, parameters)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    prediction = gridsearch.predict(X_test)\n",
    "    best_params = gridsearch.best_params_\n",
    "    best_score = gridsearch.best_estimator_.score(X_test,y_test)\n",
    "    if binary == True:\n",
    "        recall = recall_score(y_test,prediction)\n",
    "        return best_score, recall, best_params\n",
    "    else:\n",
    "        recall = recall_score(y_test,prediction, average='macro')\n",
    "        return best_score, recall, best_params\n",
    "#kNN\n",
    "#NaiveBayes\n",
    "#RandomForrest\n",
    "#LogisticRegression\n",
    "#OLS\n",
    "#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    #flight_2016, flight_2017 = data_cleaning()\n",
    "    #print(pd.unique(flight_2016['Dep_Delay']))\n",
    "    #print(flight_2016['Dep_Delay'].value_counts())\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_data(train_size=0.7, binary=True, smote_data=False)\n",
    "    binary = True\n",
    "    # print(\"Binary\")\n",
    "    # print(\"KNN\")\n",
    "    # print(best_kNN(X_train, X_test, y_train, y_test,binary))\n",
    "    # print(\"Logistic\")\n",
    "    # print(best_logistic(X_train, X_test, y_train, y_test,binary))\n",
    "    # print(\"Random Forest Classifer\")\n",
    "    # print(best_random_forest_class(X_train, X_test, y_train, y_test,binary))\n",
    "    # print(\"Elastic\")\n",
    "    # print(best_elastic(True))\n",
    "    # print(\"Gaussian\")\n",
    "    # print(best_Gaussian(X_train, X_test, y_train, y_test,binary))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_data(train_size=0.7, binary=False, smote_data=True)\n",
    "    binary = False\n",
    "    # print(\"Not Binary\")\n",
    "    # print(\"KNN\")\n",
    "    # print(best_kNN(X_train, X_test, y_train, y_test,binary))\n",
    "    print(\"Random Forest Classifer\")\n",
    "    print(best_random_forest_class(X_train, X_test, y_train, y_test,binary))\n",
    "    # print(\"Elastic\")\n",
    "    # print(best_elastic(False))\n",
    "    # print(\"Gaussian\")\n",
    "    # print(best_Gaussian(X_train, X_test, y_train, y_test,binary))\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_data(train_size=0.7, binary=False)\n",
    "    # smitten = smote(X_train[y_train==400].to_numpy(),2,2)\n",
    "    # print(smitten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
